# Opik Evaluation Playground

This project is a sandbox for testing and evaluating AI applications using [Opik](https://www.comet.com/docs/opik/quickstart), a platform for logging, monitoring, and evaluating LLM (Large Language Model) calls and chains.

## Features

- Integrates the Opik SDK for observability and evaluation of LLM applications
- Example code for logging LLM calls and chains
- Easy setup for experimenting with Opik features

## Getting Started

1. **Clone the repository:**
   ```bash
   git clone <your-repo-url>
   cd <your-project-directory>
   ```

2. **Set up a virtual environment (recommended):**
   ```bash
   python3 -m venv venv
   source venv/bin/activate
   ```

3. **Install dependencies:**
   ```bash
   pip3 install -r requirements.txt
   ```

4. **Configure Opik:**
   ```bash
   opik configure
   ```
   Follow the prompts to set up your Opik credentials.

5. **Run your experiments!**

## References

- [Opik Quickstart Guide](https://www.comet.com/docs/opik/quickstart) 